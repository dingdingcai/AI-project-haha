{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys, time\n",
    "sys.path.append('../')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from model.loss import myloss\n",
    "from model.metric import all_accuracy\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from model.DepthwiseConv2D import DepthwiseConv2D\n",
    "from model.switchnorm import SwitchNormalization\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from dataload.data_generator import *\n",
    "from model.core_model import model_v1 as matrix_model\n",
    "from model.core_model import model_v6 as point_model\n",
    "\n",
    "gpus = 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "session = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_project = os.path.abspath('../')\n",
    "data_dir    = os.path.join(base_project, 'dataspace')\n",
    "model_name = 'model_v1'\n",
    "image_size = (512, 512)\n",
    "mask_size  = (31, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 257107 image pairs.\n",
      "Found 88145 image pairs.\n",
      "Found 15907 image pairs.\n",
      "samples_train_epoch = 257107\n",
      "steps_train = 8035\n",
      "samples_valid_epoch = 88145\n",
      "steps_valid = 2755\n",
      "samples_test_epoch = 15907\n",
      "steps_test = 498\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.0 \n",
    "heatmap_height = mask_size[0]\n",
    "heatmap_width  = mask_size[1]\n",
    "\n",
    "train_batch_size = 32\n",
    "valid_batch_size = 32\n",
    "\n",
    "train_gen = MyImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    rotation_range=6,\n",
    "    channel_shift_range=15,\n",
    "    zoom_range=(.9, 1.1),\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "gen = MyImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_gen.myflow_from_directory(os.path.join(data_dir, 'train'),\n",
    "                                                  target_size       = image_size,\n",
    "                                                  x_threshold       = threshold,\n",
    "                                                  y_threshold       = threshold,\n",
    "                                                  dataset_mode      = 'valid',\n",
    "                                                  return_index_array= True,\n",
    "                                                  heatmap_height    = heatmap_height,\n",
    "                                                  heatmap_width     = heatmap_width,\n",
    "                                                  batch_size        = train_batch_size)\n",
    "\n",
    "valid_generator = gen.myflow_from_directory(os.path.join(data_dir, 'valid'), \n",
    "                                           target_size       = image_size,\n",
    "                                           x_threshold       = threshold,\n",
    "                                           y_threshold       = threshold,\n",
    "                                           dataset_mode      = 'valid',\n",
    "                                           return_index_array= True,\n",
    "                                           heatmap_height   = heatmap_height,\n",
    "                                           heatmap_width   = heatmap_width,\n",
    "                                           batch_size        = valid_batch_size)\n",
    "\n",
    "test_generator = gen.myflow_from_directory(os.path.join(data_dir, 'test'), \n",
    "                                           target_size       = image_size,\n",
    "                                           x_threshold       = threshold,\n",
    "                                           y_threshold       = threshold,\n",
    "                                           dataset_mode      = 'valid',\n",
    "                                           return_index_array= True,\n",
    "                                           heatmap_height   = heatmap_height,\n",
    "                                           heatmap_width   = heatmap_width,\n",
    "                                           batch_size        = valid_batch_size)\n",
    "\n",
    "train_samples_epoch = train_generator.data_num\n",
    "print(\"samples_train_epoch = {}\".format(train_samples_epoch))\n",
    "\n",
    "steps_train = len(train_generator)\n",
    "print(\"steps_train = {}\".format(steps_train))\n",
    "\n",
    "valid_samples_epoch = valid_generator.data_num\n",
    "print(\"samples_valid_epoch = {}\".format(valid_samples_epoch))\n",
    "\n",
    "steps_valid = len(valid_generator)\n",
    "print(\"steps_valid = {}\".format(steps_valid))\n",
    "\n",
    "test_samples_epoch = test_generator.data_num\n",
    "print(\"samples_test_epoch = {}\".format(test_samples_epoch))\n",
    "\n",
    "steps_test = len(test_generator)\n",
    "print(\"steps_test = {}\".format(steps_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(base_project,  'checkpoints/' + model_name)\n",
    "model      = load_model(os.path.join(model_path, 'matrix_model_v1_201901240447_508.2626_0.5644_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "    \n",
    "def NptoImg(x):\n",
    "    return np.uint8(np.asarray((np.squeeze(x) + 1) * 127.5))\n",
    "\n",
    "def ImgtoInput(x):\n",
    "    return np.float64(x) / 127.5 - 1\n",
    "\n",
    "def MptoImg(x):\n",
    "    mp = np.squeeze(x)\n",
    "    mp = mp - np.min(mp)\n",
    "    mp = mp / (np.max(mp) + 1e-18)\n",
    "    mp = np.uint8(mp * 255)\n",
    "    return mp\n",
    "\n",
    "def post_process_prob_argmax(y_pred, y_count, threshold=0.1, filter_size=3):\n",
    "    h, w = y_pred.shape\n",
    "    y_hat = np.zeros_like(y_pred)\n",
    "    y_count = int(y_count.round())\n",
    "    sz = int((filter_size - 1) / 2)\n",
    "    range_size = [int(v) for v in np.linspace(-sz, sz, 2*sz+1)]\n",
    "    y_pred[y_pred < threshold] = 0\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            v_max = y_pred[i,j]\n",
    "            ix_m = i\n",
    "            iy_m = j\n",
    "            flag = True\n",
    "            for i_x in range_size:\n",
    "                for i_y in range_size:\n",
    "                    ix = i_x + i\n",
    "                    iy = i_y + j\n",
    "                    if i_x + i < 0:\n",
    "                        ix = 0\n",
    "                    if i_y + j < 0:\n",
    "                        iy = 0\n",
    "                    if i_x + i > h - 1:\n",
    "                        ix = h - 1\n",
    "                    if i_y + j > w - 1:\n",
    "                        iy = w - 1\n",
    "                    v_current = y_pred[ix, iy]\n",
    "                    if v_max < v_current:\n",
    "                        flag = False\n",
    "            if flag:\n",
    "                y_hat[ix_m, iy_m] = v_max\n",
    "    \n",
    "    y_hat_flatten = y_hat.flatten()\n",
    "    y_sort_index = np.argsort(y_hat_flatten)         # ascending order\n",
    "    \n",
    "    N_largest_indice = y_sort_index[-y_count:]       # the N largest probabilities\n",
    "    N_largest_prob = y_hat_flatten[N_largest_indice]\n",
    "    y_hat_flatten[N_largest_indice] = 1\n",
    "    y_hat_pred = y_hat_flatten.reshape((h, w))\n",
    "    \n",
    "    return y_hat_pred.round(), N_largest_prob\n",
    "\n",
    "\n",
    "def add_weight(img, in_x, weight=0.4, if_filter=False, black=False):\n",
    "    cam = cv2.resize(in_x, (img.shape[0], img.shape[1]), interpolation=cv2.INTER_NEAREST)\n",
    "    if if_filter:\n",
    "        heatmap = np.tile(np.uint8(255*cam)[:, :, np.newaxis], (1, 1, 3))\n",
    "    else:\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "    out = cv2.addWeighted(img, 1.0, heatmap, weight, 0)\n",
    "    if black:\n",
    "        out[heatmap>=250] = 0\n",
    "    out = out[:, :, ::-1]\n",
    "    return out\n",
    "\n",
    "def display_masking_image(imgs, names=None, sz=20):\n",
    "    N = len(imgs)\n",
    "#     print(N)\n",
    "    if N == 1:\n",
    "        display_image_one(imgs[0])\n",
    "        if names:\n",
    "                plt.title(names[0])\n",
    "    else:\n",
    "        plt.figure(figsize=(sz, sz * N))\n",
    "        for i in range(N):\n",
    "            plt.subplot(1, N, i + 1)\n",
    "            plt.imshow(imgs[i])\n",
    "            if names:\n",
    "                plt.title(names[i])\n",
    "\n",
    "def generate_matrix(a_map, b_map):\n",
    "    a_hei, a_wid = a_map.shape\n",
    "    b_hei, b_wid = b_map.shape\n",
    "    matrix_mask = np.zeros((b_hei, b_wid, a_hei, a_wid, 1))\n",
    "    for b_h in range(b_hei):\n",
    "        for b_w in range(a_wid):\n",
    "            for a_h in range(a_hei):\n",
    "                for a_w in range(a_wid):\n",
    "                    if a_map[a_h, a_w] and b_map[b_h, b_w]:\n",
    "                        matrix_mask[b_h, b_w, a_h, a_w, :] = 1\n",
    "    return matrix_mask\n",
    "\n",
    "def mash_sku_points(sku_points, heatmap_size=(31, 31)):\n",
    "    map_sku = {}  # {index:sku}\n",
    "    point_lists = [] # [h, w]\n",
    "    im_h = heatmap_size[0]\n",
    "    im_w = heatmap_size[1]\n",
    "    sku_map = np.zeros((im_h, im_w))\n",
    "    \n",
    "    for sku, point_list in sku_points.items():\n",
    "        for point in point_list:\n",
    "            h_ratio = point[1]\n",
    "            w_ratio = point[0]\n",
    "            if 0 < h_ratio < 1 and 0 < w_ratio < 1:\n",
    "                map_row, map_col = int(h_ratio * im_h), int(w_ratio * im_w)\n",
    "                map_sku[map_row * im_h + map_col] = sku\n",
    "                point_lists.append((map_row, map_col))\n",
    "                sku_map[map_row, map_col] = 1\n",
    "    return map_sku, sku_map, point_lists\n",
    "\n",
    "def index2coord(index, heatmap_size=31):\n",
    "    h, w = divmod(index, heatmap_size)\n",
    "    return (h, w)\n",
    "\n",
    "def coord2index(map_row, map_col, im_h =31):\n",
    "    return map_row * im_h + map_col\n",
    "\n",
    "def merge_sku(sku_point_dict):\n",
    "    sku_dict = {}\n",
    "    for sku, point_list in sku_point_dict.items():\n",
    "        for point in point_list:\n",
    "            if sku in sku_dict:\n",
    "                sku_dict[sku] +=1\n",
    "            else:\n",
    "                sku_dict[sku] = 1\n",
    "    return sku_dict\n",
    "\n",
    "def cmp_sku(a_sku_dict, b_sku_dict):\n",
    "    flag = True\n",
    "    if len(a_sku_dict) != len(b_sku_dict):\n",
    "        flag = False\n",
    "    else:\n",
    "        for a_sku in a_sku_dict:\n",
    "            if a_sku not in b_sku_dict:\n",
    "                flag = False\n",
    "            elif a_sku_dict[a_sku] != b_sku_dict[a_sku]:\n",
    "                    flag = False\n",
    "    return flag\n",
    "\n",
    "def count_sku_dict(dict_list):\n",
    "    sumup = 0\n",
    "    for sku, number in dict_list.items():\n",
    "        sumup += number\n",
    "    return sumup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, index_array, a_sku_points, b_sku_points, img_path, batch_input_a_all, batch_input_b_all = test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "black = True\n",
    "\n",
    "x_img_A          = x[0][idx]\n",
    "x_img_B          = x[1][idx]\n",
    "x_matrix_mask    = x[2][idx]\n",
    "a_path           = img_path[idx][0]\n",
    "b_path           = img_path[idx][1]\n",
    "a_sku_point_dict = a_sku_points[idx]\n",
    "b_sku_point_dict = b_sku_points[idx]\n",
    "\n",
    "point_sku_A, point_mask_A, sku_coord_A = mash_sku_points(a_sku_point_dict) # {index, sku}\n",
    "point_sku_B, point_mask_B, sku_coord_B = mash_sku_points(b_sku_point_dict) # {index, sku}\n",
    "\n",
    "b_true_sku_number   = merge_sku(b_sku_point_dict) # {index, sku}\n",
    "b_true_count        = count_sku_dict(b_true_sku_number)\n",
    "x_matrix_mask       = generate_matrix(point_mask_A, point_mask_B)\n",
    "\n",
    "x_input_a           = np.expand_dims(x_img_A, 0)\n",
    "x_input_b           = np.expand_dims(x_img_B, 0)\n",
    "x_input_matrix_mask = np.expand_dims(x_matrix_mask, 0)\n",
    "\n",
    "y_pred_matrix = model.predict([x_input_a, x_input_b, x_input_matrix_mask])\n",
    "y_pred_matrix = y_pred_matrix.squeeze()\n",
    "\n",
    "img_A = cv2.imread(a_path) # BGR\n",
    "img_A = cv2.resize(img_A, image_size)\n",
    "img_B = cv2.imread(b_path) # BGR\n",
    "img_B = cv2.resize(img_B, image_size)\n",
    "\n",
    "\n",
    "black = True\n",
    "# black = False\n",
    "mask_hei = x_matrix_mask.shape[0]\n",
    "mask_wid = x_matrix_mask.shape[1]\n",
    "b_pred_sku_number = {}\n",
    "showing_num = 0\n",
    "for x_b in range(mask_hei):\n",
    "    for y_b in range(mask_wid):\n",
    "        if point_mask_B[x_b, y_b]: # B_points_mask\n",
    "            A_item_point = np.zeros((31, 31))\n",
    "            B_item_point = np.zeros((31, 31))\n",
    "            pred_similiar_points_prob = y_pred_matrix[x_b, y_b] # A similiar matrix \n",
    "            pred_similiar_points_prob = pred_similiar_points_prob * point_mask_A\n",
    "            pred_similiar_points_prob[pred_similiar_points_prob < 0.5] = 0 # skip the low confidence point\n",
    "            \n",
    "            max_index_A = pred_similiar_points_prob.argmax()\n",
    "            if max_index_A > 0:\n",
    "                \n",
    "                point_max_prob = pred_similiar_points_prob.flatten()[max_index_A]\n",
    "                point_pred_sku_id = point_sku_A[max_index_A]\n",
    "                point_true_sku_id = point_sku_B[coord2index(x_b, y_b)]\n",
    "                \n",
    "                if point_pred_sku_id in b_pred_sku_number:\n",
    "                        b_pred_sku_number[point_pred_sku_id] += 1\n",
    "                else:\n",
    "                    b_pred_sku_number[point_pred_sku_id] = 1\n",
    "            if showing_num < 15: # only show 10 pimages\n",
    "                max_hei, max_wid = index2coord(index = max_index_A, heatmap_size=mask_hei)\n",
    "                A_item_point[max_hei, max_wid] = 1\n",
    "                B_item_point[x_b, y_b] = 1\n",
    "                masking_A = add_weight(img_A, A_item_point, if_filter=True, weight=1, black=black)\n",
    "                masking_B = add_weight(img_B, B_item_point, if_filter=True, weight=1, black=black)\n",
    "                display_masking_image([masking_B, masking_A], names=[point_true_sku_id, point_pred_sku_id])\n",
    "                showing_num += 1\n",
    "\n",
    "b_pred_count = count_sku_dict(b_pred_sku_number)\n",
    "if cmp_sku(b_pred_sku_number, b_true_sku_number):\n",
    "    print('Correction!')\n",
    "else:\n",
    "    print('Wrong!')\n",
    "\n",
    "print(b_true_sku_number)\n",
    "print(b_pred_sku_number)\n",
    "print(b_true_count, b_pred_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
